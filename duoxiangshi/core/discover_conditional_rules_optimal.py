import pandas as pd
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_text
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import r2_score, accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
import numpy as np
import warnings
import argparse
from itertools import combinations
import time
warnings.filterwarnings('ignore')

class OptimalConditionalRuleDiscoverer:
    """
    ä¼˜åŒ–ç‰ˆçš„æ¡ä»¶å¤šé¡¹å¼è§„åˆ™å‘ç°å™¨
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. æ™ºèƒ½ç‰¹å¾ç»„åˆç©·ä¸¾
    2. äº¤å‰éªŒè¯è¯„ä¼°æœ€ä¼˜ç»„åˆ
    3. æ”¯æŒç¦»æ•£å‹åˆ†æ®µç‰¹å¾
    4. åŠ¨æ€ç‰¹å¾åˆ†é…ä¼˜åŒ–
    5. ğŸ†• æ”¯æŒåˆ†ç±»å‹ç›®æ ‡å˜é‡
    6. ğŸ†• æ™ºèƒ½æ£€æµ‹ç›®æ ‡å˜é‡ç±»å‹
    7. ğŸ†• æ··åˆç±»å‹ç›®æ ‡æ”¯æŒ
    """
    
    def __init__(self, max_depth=3, min_samples_leaf=50, cv_folds=3, 
                 max_combinations=100, enable_exhaustive_search=True):
        """
        åˆå§‹åŒ–å‚æ•°
        
        Args:
            max_depth: å†³ç­–æ ‘æœ€å¤§æ·±åº¦
            min_samples_leaf: å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            max_combinations: æœ€å¤§å°è¯•çš„ç‰¹å¾ç»„åˆæ•°ï¼ˆé˜²æ­¢è®¡ç®—é‡è¿‡å¤§ï¼‰
            enable_exhaustive_search: æ˜¯å¦å¯ç”¨ç©·ä¸¾æœç´¢
        """
        self.max_depth = max_depth
        self.min_samples_leaf = min_samples_leaf
        self.cv_folds = cv_folds
        self.max_combinations = max_combinations
        self.enable_exhaustive_search = enable_exhaustive_search
        self.discovered_rules = []
        self.best_configuration = None
        self.label_encoders = {}  # å­˜å‚¨åˆ†ç±»ç‰¹å¾çš„ç¼–ç å™¨
        self.categorical_features = []  # å­˜å‚¨åˆ†ç±»ç‰¹å¾åˆ—è¡¨
        self.target_encoder = None  # ğŸ†• ç›®æ ‡å˜é‡ç¼–ç å™¨
        self.target_type = None  # ğŸ†• ç›®æ ‡å˜é‡ç±»å‹ï¼š'numeric', 'categorical', 'mixed'
        self.is_classification = False  # ğŸ†• æ˜¯å¦ä¸ºåˆ†ç±»é—®é¢˜
        
    def _identify_target_type(self, data, target_col):
        """
        ğŸ†• è¯†åˆ«ç›®æ ‡å˜é‡çš„ç±»å‹
        
        Returns:
            target_type: 'numeric', 'categorical', 'mixed'
            is_classification: bool
        """
        target_values = data[target_col]
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹
        if pd.api.types.is_numeric_dtype(target_values):
            unique_count = target_values.nunique()
            
            # å¦‚æœå”¯ä¸€å€¼å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯ç¼–ç çš„åˆ†ç±»å˜é‡
            if unique_count <= 20 and unique_count < len(target_values) * 0.1:
                print(f"ğŸ” ç›®æ ‡å˜é‡ '{target_col}' è¯†åˆ«ä¸ºåˆ†ç±»å‹ï¼ˆæ•°å€¼ç¼–ç ï¼‰")
                return 'categorical', True
            else:
                print(f"ğŸ” ç›®æ ‡å˜é‡ '{target_col}' è¯†åˆ«ä¸ºæ•°å€¼å‹")
                return 'numeric', False
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç±»å‹ï¼ˆåŒ…å«å­—ç¬¦ä¸²å’Œæ•°å­—ï¼‰
            string_values = [v for v in target_values if isinstance(v, str)]
            numeric_values = [v for v in target_values if isinstance(v, (int, float))]
            
            if len(string_values) > 0 and len(numeric_values) > 0:
                print(f"ğŸ” ç›®æ ‡å˜é‡ '{target_col}' è¯†åˆ«ä¸ºæ··åˆç±»å‹ï¼ˆå­—ç¬¦ä¸²+æ•°å­—ï¼‰")
                return 'mixed', True
            elif len(string_values) > 0:
                print(f"ğŸ” ç›®æ ‡å˜é‡ '{target_col}' è¯†åˆ«ä¸ºåˆ†ç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰")
                return 'categorical', True
            else:
                print(f"ğŸ” ç›®æ ‡å˜é‡ '{target_col}' è¯†åˆ«ä¸ºæ•°å€¼å‹")
                return 'numeric', False
                
    def _prepare_target_variable(self, data, target_col):
        """
        ğŸ†• é¢„å¤„ç†ç›®æ ‡å˜é‡
        
        Returns:
            encoded_target: ç¼–ç åçš„ç›®æ ‡å˜é‡
            target_info: ç›®æ ‡å˜é‡ä¿¡æ¯
        """
        target_values = data[target_col]
        
        if self.target_type == 'mixed' or self.target_type == 'categorical':
            # éœ€è¦ç¼–ç åˆ†ç±»ç›®æ ‡
            if self.target_encoder is None:
                self.target_encoder = LabelEncoder()
                encoded_target = self.target_encoder.fit_transform(target_values.astype(str))
                
                # æ˜¾ç¤ºç¼–ç æ˜ å°„
                class_mapping = dict(zip(self.target_encoder.classes_, 
                                       self.target_encoder.transform(self.target_encoder.classes_)))
                print(f"ğŸ·ï¸ ç›®æ ‡å˜é‡ç¼–ç æ˜ å°„: {class_mapping}")
            else:
                encoded_target = self.target_encoder.transform(target_values.astype(str))
                
            target_info = {
                'type': self.target_type,
                'num_classes': len(self.target_encoder.classes_),
                'classes': self.target_encoder.classes_,
                'is_encoded': True
            }
            
            return encoded_target, target_info
        else:
            # æ•°å€¼å‹ç›®æ ‡ï¼Œæ— éœ€ç¼–ç 
            target_info = {
                'type': 'numeric',
                'is_encoded': False
            }
            return target_values.values, target_info
        
    def _identify_feature_types(self, data, target_col):
        """
        è¯†åˆ«æ•°å€¼å‹å’Œåˆ†ç±»å‹ç‰¹å¾
        
        Returns:
            numeric_features: æ•°å€¼å‹ç‰¹å¾åˆ—è¡¨
            categorical_features: åˆ†ç±»å‹ç‰¹å¾åˆ—è¡¨
            all_split_candidates: æ‰€æœ‰å¯ç”¨ä½œåˆ†æ®µçš„ç‰¹å¾
        """
        # æ’é™¤ç›®æ ‡åˆ—
        feature_cols = [col for col in data.columns if col != target_col]
        
        numeric_features = []
        categorical_features = []
        
        for col in feature_cols:
            if pd.api.types.is_numeric_dtype(data[col]):
                numeric_features.append(col)
            else:
                # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»ç‰¹å¾ï¼ˆå­—ç¬¦ä¸²ã€å¯¹è±¡ç±»å‹ï¼Œæˆ–æ•°å€¼ä½†å”¯ä¸€å€¼è¾ƒå°‘ï¼‰
                if data[col].dtype == 'object' or data[col].dtype.name == 'category':
                    categorical_features.append(col)
                elif pd.api.types.is_numeric_dtype(data[col]) and data[col].nunique() <= 10:
                    # æ•°å€¼å‹ä½†å”¯ä¸€å€¼è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯ç¼–ç çš„åˆ†ç±»ç‰¹å¾
                    print(f"å°†æ•°å€¼ç‰¹å¾ '{col}' è§†ä¸ºåˆ†ç±»ç‰¹å¾ï¼ˆå”¯ä¸€å€¼æ•°é‡: {data[col].nunique()}ï¼‰")
                    categorical_features.append(col)
                else:
                    numeric_features.append(col)
        
        # æ‰€æœ‰ç‰¹å¾éƒ½å¯ä»¥ä½œä¸ºåˆ†æ®µå€™é€‰ï¼ˆåˆ†ç±»ç‰¹å¾ç”¨äºåˆ†æ®µï¼Œæ•°å€¼ç‰¹å¾æ—¢å¯åˆ†æ®µä¹Ÿå¯åšå¤šé¡¹å¼ï¼‰
        all_split_candidates = numeric_features + categorical_features
        
        print(f"ç‰¹å¾ç±»å‹è¯†åˆ«:")
        print(f"  æ•°å€¼ç‰¹å¾: {numeric_features}")
        print(f"  åˆ†ç±»ç‰¹å¾: {categorical_features}")
        print(f"  å¯åˆ†æ®µç‰¹å¾: {all_split_candidates}")
        
        return numeric_features, categorical_features, all_split_candidates
        
    def _encode_categorical_features(self, data, categorical_features):
        """
        å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
        
        Returns:
            encoded_data: ç¼–ç åçš„æ•°æ®
        """
        encoded_data = data.copy()
        
        for col in categorical_features:
            if col not in self.label_encoders:
                # åˆ›å»ºå¹¶æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨
                le = LabelEncoder()
                encoded_data[col] = le.fit_transform(data[col].astype(str))
                self.label_encoders[col] = le
                print(f"å¯¹åˆ†ç±»ç‰¹å¾ '{col}' è¿›è¡Œç¼–ç : {dict(zip(le.classes_, le.transform(le.classes_)))}")
            else:
                # ä½¿ç”¨å·²æœ‰çš„ç¼–ç å™¨
                encoded_data[col] = self.label_encoders[col].transform(data[col].astype(str))
        
        return encoded_data
    
    def _generate_feature_combinations(self, numeric_features, categorical_features, target_col, effective_poly_candidates):
        """
        ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾ç»„åˆ
        
        Args:
            numeric_features: æ•°å€¼ç‰¹å¾åˆ—è¡¨
            categorical_features: åˆ†ç±»ç‰¹å¾åˆ—è¡¨
            target_col: ç›®æ ‡åˆ—å
            effective_poly_candidates: æœ‰æ•ˆå¤šé¡¹å¼ç‰¹å¾å€™é€‰åˆ—è¡¨
            
        Returns:
            combinations_list: [(split_features, poly_features), ...]
        """
        # åˆ†æ®µç‰¹å¾å€™é€‰ï¼šæ•°å€¼ç‰¹å¾ + åˆ†ç±»ç‰¹å¾
        split_candidates = numeric_features + categorical_features
        # å¤šé¡¹å¼ç‰¹å¾å€™é€‰ï¼šåªèƒ½æ˜¯æ•°å€¼ç‰¹å¾
        poly_candidates = effective_poly_candidates
        
        if len(split_candidates) < 1 or len(poly_candidates) < 1:
            print("è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„ç‰¹å¾è¿›è¡Œç»„åˆä¼˜åŒ–")
            return [([], poly_candidates)]
        
        combinations_list = []
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„åˆ†æ®µç‰¹å¾ç»„åˆ
        for split_size in range(1, min(len(split_candidates), 4) + 1):  # é™åˆ¶åˆ†æ®µç‰¹å¾æœ€å¤š4ä¸ª
            for split_features in combinations(split_candidates, split_size):
                # å¤šé¡¹å¼ç‰¹å¾ï¼šæ•°å€¼ç‰¹å¾ä¸­å»é™¤å·²ç”¨ä½œåˆ†æ®µçš„æ•°å€¼ç‰¹å¾
                available_poly_features = [f for f in poly_candidates if f not in split_features]
                
                # ç¡®ä¿å¤šé¡¹å¼ç‰¹å¾è‡³å°‘æœ‰1ä¸ª
                if len(available_poly_features) >= 1:
                    combinations_list.append((list(split_features), available_poly_features))
        
        # å¦‚æœç»„åˆæ•°é‡å¤ªå¤šï¼Œè¿›è¡Œæ™ºèƒ½ç­›é€‰
        if len(combinations_list) > self.max_combinations:
            print(f"è­¦å‘Š: ç‰¹å¾ç»„åˆæ•°é‡ ({len(combinations_list)}) è¶…è¿‡æœ€å¤§é™åˆ¶ ({self.max_combinations})")
            print("å°†ä½¿ç”¨å¯å‘å¼æ–¹æ³•ç­›é€‰æœ€æœ‰å¸Œæœ›çš„ç»„åˆ...")
            combinations_list = self._select_promising_combinations(combinations_list, split_candidates, poly_candidates)
        
        return combinations_list
    
    def _select_promising_combinations(self, all_combinations, split_candidates, poly_candidates):
        """
        ä½¿ç”¨å¯å‘å¼æ–¹æ³•ç­›é€‰æœ€æœ‰å¸Œæœ›çš„ç‰¹å¾ç»„åˆ
        """
        # ç­–ç•¥1: ä¼˜å…ˆé€‰æ‹©åˆ†æ®µç‰¹å¾æ•°é‡é€‚ä¸­çš„ç»„åˆ (1-2ä¸ªåˆ†æ®µç‰¹å¾)
        moderate_combinations = [combo for combo in all_combinations 
                               if 1 <= len(combo[0]) <= 2]
        
        # ç­–ç•¥2: ä¼˜å…ˆé€‰æ‹©åŒ…å«åˆ†ç±»ç‰¹å¾çš„ç»„åˆï¼ˆåˆ†ç±»ç‰¹å¾é€šå¸¸æ˜¯å¥½çš„åˆ†æ®µç‰¹å¾ï¼‰
        categorical_combinations = [combo for combo in moderate_combinations
                                  if any(f in self.categorical_features for f in combo[0])]
        
        # ç­–ç•¥3: å¦‚æœåˆ†ç±»ç‰¹å¾ç»„åˆä¸å¤Ÿï¼Œè¡¥å……æ•°å€¼ç‰¹å¾ç»„åˆ
        if len(categorical_combinations) < self.max_combinations // 2:
            remaining_combinations = [combo for combo in moderate_combinations 
                                    if combo not in categorical_combinations]
            categorical_combinations.extend(remaining_combinations[:self.max_combinations // 2])
        
        # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œéšæœºé‡‡æ ·
        if len(categorical_combinations) > self.max_combinations:
            np.random.seed(42)
            indices = np.random.choice(len(categorical_combinations), 
                                     self.max_combinations, replace=False)
            categorical_combinations = [categorical_combinations[i] for i in indices]
        
        # ç­–ç•¥5: æ€»æ˜¯åŒ…å«ä¸€äº›åŸºå‡†ç»„åˆ
        baseline_combinations = []
        if split_candidates and poly_candidates:
            baseline_combinations = [
                ([split_candidates[0]], poly_candidates),  # ç¬¬ä¸€ä¸ªç‰¹å¾ä½œä¸ºåˆ†æ®µ
            ]
            if len(split_candidates) > 1:
                baseline_combinations.append(([split_candidates[-1]], [f for f in poly_candidates if f != split_candidates[-1]]))
        
        # åˆå¹¶å¹¶å»é‡
        final_combinations = baseline_combinations + categorical_combinations
        seen = set()
        unique_combinations = []
        for combo in final_combinations:
            combo_key = (tuple(sorted(combo[0])), tuple(sorted(combo[1])))
            if combo_key not in seen:
                seen.add(combo_key)
                unique_combinations.append(combo)
        
        return unique_combinations[:self.max_combinations]
    
    def _detect_simple_categorical_mapping(self, data, target_col, encoded_target, target_info):
        """
        ğŸ†• æ£€æµ‹ç®€å•çš„åˆ†ç±»æ˜ å°„è§„åˆ™ï¼ˆå¦‚ lisan.csv ä¸­çš„è§„å¾‹ï¼‰
        
        ä¸“é—¨å¤„ç†å½¢å¦‚ï¼š
        - å½“ x=x1 æ—¶ï¼Œresult = açš„å€¼
        - å½“ x=x2 æ—¶ï¼Œresult = bçš„å€¼
        - å½“ x=x3 æ—¶ï¼Œresult = cçš„å€¼
        
        Returns:
            simple_rules: å‘ç°çš„ç®€å•æ˜ å°„è§„åˆ™åˆ—è¡¨
        """
        if not self.is_classification:
            return []
            
        print("ğŸ” å°è¯•æ£€æµ‹ç®€å•åˆ†ç±»æ˜ å°„è§„åˆ™...")
        simple_rules = []
        
        # è·å–æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆé™¤ç›®æ ‡åˆ—å¤–ï¼‰
        feature_cols = [col for col in data.columns if col != target_col]
        
        # å°è¯•æ¯ä¸ªåˆ†ç±»ç‰¹å¾ä½œä¸ºä¸»åˆ†æ®µç‰¹å¾
        for primary_feature in self.categorical_features:
            if primary_feature not in data.columns:
                continue
                
            primary_values = sorted(data[primary_feature].unique())
            print(f"   æ£€æµ‹ä¸»ç‰¹å¾ '{primary_feature}' çš„å€¼: {primary_values}")
            
            # å¯¹äºæ¯ä¸ªä¸»ç‰¹å¾å€¼ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„æ˜ å°„ç‰¹å¾
            segment_rules = []
            total_coverage = 0
            
            for primary_val in primary_values:
                # è·å–è¯¥åˆ†æ®µçš„æ•°æ®
                segment_mask = data[primary_feature] == primary_val
                segment_data = data[segment_mask]
                segment_target = encoded_target[segment_mask]
                
                if len(segment_data) == 0:
                    continue
                
                # å°è¯•æ¯ä¸ªå…¶ä»–ç‰¹å¾ä½œä¸ºæ˜ å°„ç‰¹å¾
                best_mapping_rule = None
                best_accuracy = 0
                
                for mapping_feature in feature_cols:
                    if mapping_feature == primary_feature:
                        continue
                        
                    # æµ‹è¯•æ˜¯å¦ result = mapping_feature çš„å€¼
                    try:
                        # è·å–å®é™…ç›®æ ‡å€¼ï¼ˆè§£ç å›åŸå§‹å€¼ï¼‰
                        actual_values = [self.target_encoder.classes_[int(t)] for t in segment_target]
                        
                        # è·å–æ˜ å°„ç‰¹å¾çš„å€¼å¹¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        predicted_values = [str(v) for v in segment_data[mapping_feature].values]
                        
                        # è®¡ç®—å‡†ç¡®ç‡
                        correct_predictions = sum(str(p) == str(a) for p, a in zip(predicted_values, actual_values))
                        accuracy = correct_predictions / len(segment_data)
                        
                        if accuracy > best_accuracy and accuracy >= 0.8:  # è‡³å°‘80%å‡†ç¡®ç‡
                            best_accuracy = accuracy
                            best_mapping_rule = {
                                'primary_feature': primary_feature,
                                'primary_value': primary_val,
                                'mapping_feature': mapping_feature,
                                'accuracy': accuracy,
                                'sample_count': len(segment_data)
                            }
                            
                    except Exception as e:
                        continue
                
                if best_mapping_rule:
                    segment_rules.append(best_mapping_rule)
                    total_coverage += best_mapping_rule['sample_count']
                    print(f"      âœ… {primary_feature}={primary_val} â†’ result={best_mapping_rule['mapping_feature']} (å‡†ç¡®ç‡: {best_accuracy:.3f}, æ ·æœ¬: {best_mapping_rule['sample_count']})")
                else:
                    print(f"      âŒ {primary_feature}={primary_val} â†’ æœªæ‰¾åˆ°æœ‰æ•ˆæ˜ å°„")
            
            # å¦‚æœè¿™ä¸ªä¸»ç‰¹å¾çš„è¦†ç›–ç‡è¶³å¤Ÿå¥½ï¼Œä¿å­˜è§„åˆ™
            coverage_rate = total_coverage / len(data)
            if coverage_rate >= 0.7 and len(segment_rules) >= 2:  # é™ä½è¦†ç›–ç‡é˜ˆå€¼åˆ°70%ï¼Œè‡³å°‘2æ¡è§„åˆ™
                print(f"   âœ… ä¸»ç‰¹å¾ '{primary_feature}' è¦†ç›–ç‡: {coverage_rate:.1%}ï¼Œå‘ç° {len(segment_rules)} æ¡è§„åˆ™")
                
                # è½¬æ¢ä¸ºæ ‡å‡†è§„åˆ™æ ¼å¼
                for rule_info in segment_rules:
                    condition = f"{rule_info['primary_feature']} âˆˆ {{{rule_info['primary_value']}}}"
                    rule_formula = f"result = {rule_info['mapping_feature']}"
                    
                    rule = {
                        'condition': condition,
                        'rule': rule_formula,
                        'score': rule_info['accuracy'],
                        'sample_count': rule_info['sample_count'],
                        'rule_type': 'classification',
                        'target_value': rule_info['mapping_feature'],  # æ ‡è®°æ˜ å°„ç‰¹å¾
                        'mapping_type': 'simple_categorical'  # æ ‡è®°ä¸ºç®€å•åˆ†ç±»æ˜ å°„
                    }
                    simple_rules.append(rule)
                    
                # æ‰¾åˆ°ä¸€ä¸ªå¥½çš„ä¸»ç‰¹å¾å°±ç»“æŸï¼Œé¿å…é‡å¤
                break
            else:
                print(f"   âŒ ä¸»ç‰¹å¾ '{primary_feature}' è¦†ç›–ç‡ä¸è¶³: {coverage_rate:.1%}")
        
        if simple_rules:
            print(f"ğŸ‰ å‘ç° {len(simple_rules)} æ¡ç®€å•åˆ†ç±»æ˜ å°„è§„åˆ™")
        else:
            print("âŒ æœªå‘ç°ç®€å•åˆ†ç±»æ˜ å°„è§„åˆ™")
            
        return simple_rules
    
    def _evaluate_combination(self, data, split_features, poly_features, target_col, encoded_target, target_info):
        """
        ğŸ†• è¯„ä¼°å•ä¸ªç‰¹å¾ç»„åˆçš„æ•ˆæœï¼ˆæ”¯æŒåˆ†ç±»å’Œå›å½’ï¼‰
        
        Returns:
            score: è¯¥ç»„åˆçš„ç»¼åˆè¯„åˆ†
            rules: å‘ç°çš„è§„åˆ™åˆ—è¡¨
        """
        try:
            X_split = data[split_features]
            y_target = encoded_target
            
            # ğŸ†• æ ¹æ®ç›®æ ‡ç±»å‹é€‰æ‹©å†³ç­–æ ‘
            if self.is_classification:
                tree_model = DecisionTreeClassifier(
                    max_depth=self.max_depth,
                    min_samples_leaf=self.min_samples_leaf,
                    random_state=42
                )
            else:
                tree_model = DecisionTreeRegressor(
                    max_depth=self.max_depth,
                    min_samples_leaf=self.min_samples_leaf,
                    random_state=42
                )
            
            tree_model.fit(X_split, y_target)
            
            # æå–æ¡ä»¶å’Œåˆ†æ®µ
            conditions_by_leaf = self._extract_tree_conditions(tree_model, split_features, data)
            leaf_ids = tree_model.apply(X_split)
            
            segment_scores = []
            segment_rules = []
            
            # è¯„ä¼°æ¯ä¸ªåˆ†æ®µçš„æ‹Ÿåˆæ•ˆæœ
            for leaf_id, conditions in conditions_by_leaf.items():
                subset_mask = (leaf_ids == leaf_id)
                subset_data = data[subset_mask]
                subset_target = y_target[subset_mask]
                
                if len(subset_data) < self.min_samples_leaf // 2:
                    continue
                
                if self.is_classification:
                    # ğŸ†• åˆ†ç±»é—®é¢˜å¤„ç†
                    rule_result = self._handle_classification_segment(
                        subset_data, subset_target, poly_features, target_col, 
                        target_info, conditions
                    )
                else:
                    # åŸæœ‰å›å½’é—®é¢˜å¤„ç†
                    rule_result = self._handle_regression_segment(
                        subset_data, subset_target, poly_features, target_col, conditions
                    )
                
                if rule_result is not None:
                    segment_scores.append(rule_result['score'])
                    segment_rules.append(rule_result)
            
            # è®¡ç®—æ•´ä½“è¯„åˆ†
            if segment_scores:
                avg_score = np.mean(segment_scores)
                rule_count_bonus = min(len(segment_scores) / 10, 0.1)
                total_score = avg_score + rule_count_bonus
                return total_score, segment_rules
            else:
                return 0.0, []
                
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼°ç»„åˆæ—¶å‡ºé”™: {e}")
            return 0.0, []
    
    def _handle_classification_segment(self, subset_data, subset_target, poly_features, target_col, target_info, conditions):
        """
        ğŸ†• å¤„ç†åˆ†ç±»é—®é¢˜çš„åˆ†æ®µ
        """
        try:
            # æ£€æŸ¥è¯¥åˆ†æ®µæ˜¯å¦ä¸ºçº¯å‡€åˆ†æ®µï¼ˆæ‰€æœ‰æ ·æœ¬çš„ç›®æ ‡å€¼ç›¸åŒï¼‰
            unique_targets = np.unique(subset_target)
            
            if len(unique_targets) == 1:
                # çº¯å‡€åˆ†æ®µï¼šç›´æ¥æ˜ å°„è§„åˆ™
                target_encoded = unique_targets[0]
                target_original = self.target_encoder.inverse_transform([target_encoded])[0]
                
                # ğŸ†• ç”Ÿæˆåˆ†ç±»è§„åˆ™
                if len(poly_features) > 0:
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç®€å•çš„æ˜ å°„å…³ç³»
                    rule_str = self._find_classification_mapping(subset_data, poly_features, target_original)
                else:
                    rule_str = f"{target_col} = {target_original}"
                
                condition_str = " ä¸” ".join(conditions)
                condition_str = self._simplify_condition_string(condition_str)
                
                return {
                    'split_features': conditions,
                    'poly_features': poly_features,
                    'condition': condition_str,
                    'rule': rule_str,
                    'score': 1.0,  # çº¯å‡€åˆ†æ®µå¾—æ»¡åˆ†
                    'sample_count': len(subset_data),
                    'rule_type': 'classification',
                    'target_value': target_original
                }
            else:
                # æ··åˆåˆ†æ®µï¼šå°è¯•æ‰¾åˆ°å±€éƒ¨æ¨¡å¼
                if len(poly_features) > 0:
                    # å°è¯•ä½¿ç”¨å¤šæ•°æŠ•ç¥¨æˆ–å…¶ä»–åˆ†ç±»æ–¹æ³•
                    most_common_target = np.bincount(subset_target).argmax()
                    target_original = self.target_encoder.inverse_transform([most_common_target])[0]
                    accuracy = np.mean(subset_target == most_common_target)
                    
                    if accuracy >= 0.7:  # è‡³å°‘70%å‡†ç¡®ç‡æ‰è®¤ä¸ºæœ‰æ•ˆ
                        rule_str = self._find_classification_mapping(subset_data, poly_features, target_original)
                        condition_str = " ä¸” ".join(conditions)
                        condition_str = self._simplify_condition_string(condition_str)
                        
                        return {
                            'split_features': conditions,
                            'poly_features': poly_features,
                            'condition': condition_str,
                            'rule': rule_str,
                            'score': accuracy,
                            'sample_count': len(subset_data),
                            'rule_type': 'classification',
                            'target_value': target_original
                        }
                
                return None
                
        except Exception as e:
            print(f"âš ï¸ å¤„ç†åˆ†ç±»åˆ†æ®µæ—¶å‡ºé”™: {e}")
            return None
    
    def _find_classification_mapping(self, subset_data, poly_features, target_value):
        """
        ğŸ†• å¯»æ‰¾åˆ†ç±»æ˜ å°„å…³ç³»
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç®€å•çš„ç›´æ¥æ˜ å°„å…³ç³»
        for feature in poly_features:
            feature_values = subset_data[feature].unique()
            
            if len(feature_values) == 1:
                # å¦‚æœè¯¥ç‰¹å¾åœ¨è¿™ä¸ªåˆ†æ®µä¸­åªæœ‰ä¸€ä¸ªå€¼ï¼Œå¯èƒ½å­˜åœ¨ç›´æ¥æ˜ å°„
                feature_value = feature_values[0]
                return f"result = {feature}"
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç®€å•æ˜ å°„ï¼Œè¿”å›ç›´æ¥èµ‹å€¼
        return f"result = {target_value}"
    
    def _handle_regression_segment(self, subset_data, subset_target, poly_features, target_col, conditions):
        """
        ğŸ†• å¤„ç†å›å½’é—®é¢˜çš„åˆ†æ®µï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        """
        try:
            if len(poly_features) == 0:
                return None
                
            X_poly = subset_data[poly_features]
            y_poly = subset_target
            
            # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°å¤šé¡¹å¼æ‹Ÿåˆæ•ˆæœ
            if len(X_poly) >= self.cv_folds:
                model = LinearRegression()
                cv_scores = cross_val_score(model, X_poly, y_poly, 
                                           cv=min(self.cv_folds, len(X_poly)), 
                                           scoring='r2')
                avg_score = np.mean(cv_scores)
                
                if avg_score > 0.1:  # æœ€å°è´¨é‡é˜ˆå€¼
                    model.fit(X_poly, y_poly)
                    rule_str = self._format_polynomial_rule(model, poly_features, target_col)
                    condition_str = " ä¸” ".join(conditions)
                    condition_str = self._simplify_condition_string(condition_str)
                    
                    return {
                        'split_features': conditions,
                        'poly_features': poly_features,
                        'condition': condition_str,
                        'rule': rule_str,
                        'score': avg_score,
                        'sample_count': len(subset_data),
                        'rule_type': 'regression',
                        'model': model
                    }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å›å½’åˆ†æ®µæ—¶å‡ºé”™: {e}")
            return None
    
    def _extract_tree_conditions(self, tree_model, feature_names, original_data):
        """
        ä»å†³ç­–æ ‘ä¸­æå–ç²¾ç¡®çš„æ¡ä»¶è·¯å¾„ï¼Œå¹¶å°†ç¼–ç åçš„åˆ†ç±»å€¼è½¬æ¢å›åŸå§‹å€¼
        """
        tree = tree_model.tree_
        conditions_by_leaf = {}
        
        def extract_path(node_id, conditions):
            if tree.children_left[node_id] == -1:  # å¶å­èŠ‚ç‚¹
                conditions_by_leaf[node_id] = conditions.copy()
                return
                
            feature_idx = tree.feature[node_id]
            threshold = tree.threshold[node_id]
            feature_name = feature_names[feature_idx]
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†ç±»ç‰¹å¾
            if feature_name in self.categorical_features:
                # åˆ†ç±»ç‰¹å¾ï¼šå°†é˜ˆå€¼è½¬æ¢å›åŸå§‹åˆ†ç±»å€¼
                le = self.label_encoders[feature_name]
                
                # è·å–è¯¥ç‰¹å¾çš„æ‰€æœ‰å¯èƒ½å€¼
                unique_encoded_values = sorted(original_data[feature_name].unique())
                
                # æ ¹æ®é˜ˆå€¼ç¡®å®šåˆ†ç±»æ¡ä»¶
                left_values = [val for val in unique_encoded_values if val <= threshold]
                right_values = [val for val in unique_encoded_values if val > threshold]
                
                # è½¬æ¢å›åŸå§‹åˆ†ç±»å€¼
                if left_values:
                    left_original = [le.inverse_transform([int(val)])[0] for val in left_values]
                    left_condition = f"{feature_name} âˆˆ {{{', '.join(map(str, left_original))}}}"
                else:
                    left_condition = f"{feature_name} âˆˆ {{}}"
                
                if right_values:
                    right_original = [le.inverse_transform([int(val)])[0] for val in right_values]
                    right_condition = f"{feature_name} âˆˆ {{{', '.join(map(str, right_original))}}}"
                else:
                    right_condition = f"{feature_name} âˆˆ {{}}"
                
                # å·¦åˆ†æ”¯
                left_conditions = conditions + [left_condition]
                extract_path(tree.children_left[node_id], left_conditions)
                
                # å³åˆ†æ”¯
                right_conditions = conditions + [right_condition]
                extract_path(tree.children_right[node_id], right_conditions)
            else:
                # æ•°å€¼ç‰¹å¾ï¼šä¿æŒåŸæœ‰é€»è¾‘
                left_conditions = conditions + [f"{feature_name} <= {threshold:.2f}"]
                extract_path(tree.children_left[node_id], left_conditions)
                
                right_conditions = conditions + [f"{feature_name} > {threshold:.2f}"]
                extract_path(tree.children_right[node_id], right_conditions)
        
        extract_path(0, [])
        return conditions_by_leaf
    
    def _format_polynomial_rule(self, model, poly_features, target_col):
        """æ ¼å¼åŒ–å¤šé¡¹å¼è§„åˆ™"""
        coefficients = model.coef_
        intercept = model.intercept_
        
        rule_parts = []
        for i, col_name in enumerate(poly_features):
            coeff_val = coefficients[i]
            
            if abs(coeff_val) < 0.001:  # ç³»æ•°æ¥è¿‘0ï¼Œå¿½ç•¥æ­¤é¡¹
                continue
                
            coeff_rounded = self._smart_round(coeff_val)
            
            if coeff_rounded == 1:
                rule_parts.append(col_name)
            elif coeff_rounded == -1:
                rule_parts.append(f"-{col_name}")
            else:
                rule_parts.append(f"{coeff_rounded} * {col_name}")
        
        intercept_rounded = self._smart_round(intercept)
        
        if not rule_parts:
            return f"{target_col} = {intercept_rounded}"
            
        rule_str = f"{target_col} = {' + '.join(rule_parts)}"
        
        if intercept_rounded != 0:
            if intercept_rounded > 0:
                rule_str += f" + {intercept_rounded}"
            else:
                rule_str += f" - {abs(intercept_rounded)}"
                
        return rule_str
    
    def _smart_round(self, value):
        """æ™ºèƒ½å››èˆäº”å…¥"""
        rounded_int = round(value)
        rounded_dec1 = round(value, 1)
        rounded_dec2 = round(value, 2)
        
        if abs(value - rounded_int) < 0.001:
            return rounded_int
        elif abs(value - rounded_dec1) < 0.001:
            return rounded_dec1
        elif abs(value - rounded_dec2) < 0.001:
            return rounded_dec2
        else:
            return round(value, 4)
    
    def _simplify_condition_string(self, condition_str):
        """
        ç®€åŒ–æ¡ä»¶å­—ç¬¦ä¸²ä¸­çš„å†—ä½™æ¡ä»¶
        
        ğŸ”§ ä¿®å¤è¯´æ˜ï¼š
        æš‚æ—¶ç¦ç”¨è‡ªåŠ¨æ¡ä»¶ç®€åŒ–ï¼Œå› ä¸ºè¿™å¯èƒ½å¯¼è‡´é‡è¦çº¦æŸçš„ä¸¢å¤±ã€‚
        å†³ç­–æ ‘ç”Ÿæˆçš„æ¡ä»¶åº”è¯¥ä¿æŒåŸæ ·ï¼Œç¡®ä¿æ¯ä¸ªåˆ†æ®µéƒ½æœ‰å®Œæ•´çš„çº¦æŸã€‚
        
        Args:
            condition_str: åŸå§‹æ¡ä»¶å­—ç¬¦ä¸²
            
        Returns:
            simplified_str: å¤„ç†åçš„æ¡ä»¶å­—ç¬¦ä¸²
        """
        if not condition_str or condition_str.strip() == "":
            return condition_str
        
        # ğŸ”§ ä¸´æ—¶ç¦ç”¨å¤æ‚çš„æ¡ä»¶åˆå¹¶é€»è¾‘ï¼Œç›´æ¥è¿”å›åŸå§‹æ¡ä»¶
        # è¿™æ ·å¯ä»¥ç¡®ä¿å†³ç­–æ ‘ç”Ÿæˆçš„æ¯ä¸ªæ¡ä»¶éƒ½è¢«å®Œæ•´ä¿ç•™
        
        # åªè¿›è¡ŒåŸºæœ¬çš„æ ¼å¼æ¸…ç†
        cleaned = condition_str.strip()
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        import re
        cleaned = re.sub(r'\s+', ' ', cleaned)
        cleaned = re.sub(r'\s*ä¸”\s*', ' ä¸” ', cleaned)
        
        return cleaned
        
        # æ³¨é‡Šæ‰åŸæœ‰çš„å¤æ‚é€»è¾‘ï¼Œé¿å…äº§ç”ŸçŸ›ç›¾æ¡ä»¶
        # # è§£ææ¡ä»¶
        # conditions = self._parse_condition(condition_str)
        # 
        # # é‡æ–°æ ¼å¼åŒ–ï¼ˆè¿™ä¼šè‡ªåŠ¨ç®€åŒ–å†—ä½™æ¡ä»¶ï¼‰
        # simplified_str = self._format_merged_conditions(conditions)
        # 
        # return simplified_str
    
    def discover_optimal_rules(self, csv_file_path, target_col=None, 
                              manual_split_features=None, manual_poly_features=None):
        """
        å‘ç°æœ€ä¼˜çš„æ¡ä»¶å¤šé¡¹å¼è§„åˆ™
        
        Args:
            csv_file_path: CSVæ–‡ä»¶è·¯å¾„
            target_col: ç›®æ ‡åˆ—åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€åä¸€åˆ—
            manual_split_features: æ‰‹åŠ¨æŒ‡å®šçš„åˆ†æ®µç‰¹å¾
            manual_poly_features: æ‰‹åŠ¨æŒ‡å®šçš„å¤šé¡¹å¼ç‰¹å¾
            
        Returns:
            best_rules: æœ€ä¼˜è§„åˆ™åˆ—è¡¨
        """
        try:
            print("=== ä¼˜åŒ–ç‰ˆæ¡ä»¶è§„åˆ™å‘ç°ï¼ˆæ”¯æŒåˆ†ç±»ç‰¹å¾+åˆ†ç±»ç›®æ ‡ï¼‰===")
            start_time = time.time()
            
            # 1. æ•°æ®åŠ è½½
            data = pd.read_csv(csv_file_path)
            print(f"æˆåŠŸåŠ è½½æ•°æ®: {csv_file_path}")
            print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
            
            # 2. ç¡®å®šç›®æ ‡åˆ—
            if target_col is None:
                target_col = data.columns[-1]
                print(f"è‡ªåŠ¨é€‰æ‹©æœ€åä¸€åˆ— '{target_col}' ä½œä¸ºç›®æ ‡åˆ—")
            
            # ğŸ†• 3. è¯†åˆ«ç›®æ ‡å˜é‡ç±»å‹
            self.target_type, self.is_classification = self._identify_target_type(data, target_col)
            
            # ğŸ†• 4. é¢„å¤„ç†ç›®æ ‡å˜é‡
            encoded_target, target_info = self._prepare_target_variable(data, target_col)
            
            # åœ¨æ•°æ®ä¸­æ·»åŠ ç¼–ç åçš„ç›®æ ‡åˆ—ç”¨äºåç»­å¤„ç†
            data_with_encoded_target = data.copy()
            data_with_encoded_target[target_col + '_encoded'] = encoded_target
            
            # 5. è¯†åˆ«ç‰¹å¾ç±»å‹
            numeric_features, categorical_features, all_split_candidates = self._identify_feature_types(data, target_col)
            self.categorical_features = categorical_features
            
            if len(all_split_candidates) < 1:
                print("é”™è¯¯: æ²¡æœ‰è¶³å¤Ÿçš„ç‰¹å¾è¿›è¡Œåˆ†æ")
                return []
            
            # ğŸ†• 6. æ ¹æ®ç›®æ ‡ç±»å‹è°ƒæ•´å¤šé¡¹å¼ç‰¹å¾ç­–ç•¥
            if self.is_classification:
                print("ğŸ”§ åˆ†ç±»é—®é¢˜ï¼šå…è®¸æ‰€æœ‰ç‰¹å¾ä½œä¸º'å¤šé¡¹å¼'ç‰¹å¾ï¼ˆå®é™…ä¸ºæ˜ å°„ç‰¹å¾ï¼‰")
                # å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œæ‰€æœ‰ç‰¹å¾éƒ½å¯ä»¥ä½œä¸º"å¤šé¡¹å¼"ç‰¹å¾ï¼ˆå®é™…æ˜¯æ˜ å°„ç‰¹å¾ï¼‰
                effective_poly_candidates = numeric_features + categorical_features
            else:
                print("ğŸ”§ å›å½’é—®é¢˜ï¼šä»…æ•°å€¼ç‰¹å¾å¯ä½œä¸ºå¤šé¡¹å¼ç‰¹å¾")
                effective_poly_candidates = numeric_features
                
            if len(effective_poly_candidates) < 1:
                print("é”™è¯¯: æ²¡æœ‰è¶³å¤Ÿçš„å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œåˆ†æ")
                return []
            
            # 7. å¯¹åˆ†ç±»ç‰¹å¾è¿›è¡Œç¼–ç 
            encoded_data = self._encode_categorical_features(data_with_encoded_target, categorical_features)
            
            # ğŸ†• 8. å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œå…ˆå°è¯•ç®€å•æ˜ å°„æ£€æµ‹
            simple_mapping_rules = []
            if self.is_classification:
                simple_mapping_rules = self._detect_simple_categorical_mapping(data, target_col, encoded_target, target_info)
                
                # å¦‚æœç®€å•æ˜ å°„è§„åˆ™è¦†ç›–ç‡è¶³å¤Ÿé«˜ï¼Œå¯ä»¥è·³è¿‡å¤æ‚æœç´¢
                if simple_mapping_rules:
                    total_simple_coverage = sum(rule['sample_count'] for rule in simple_mapping_rules)
                    simple_coverage_rate = total_simple_coverage / len(data)
                    
                    print(f"ğŸ“Š ç®€å•æ˜ å°„è§„åˆ™è¦†ç›–ç‡: {simple_coverage_rate:.1%}")
                    
                    if simple_coverage_rate >= 0.7:  # é™ä½é˜ˆå€¼åˆ°70%
                        print("ğŸ‰ ç®€å•æ˜ å°„è§„åˆ™è¦†ç›–ç‡è¶³å¤Ÿé«˜ï¼Œè·³è¿‡å¤æ‚æœç´¢")
                        
                        # è®¾ç½®æœ€ä½³é…ç½®
                        self.best_configuration = {
                            'split_features': ['simple_categorical_mapping'],
                            'poly_features': ['all_features'],
                            'score': sum(rule['score'] for rule in simple_mapping_rules) / len(simple_mapping_rules),
                            'num_rules': len(simple_mapping_rules),
                            'target_type': self.target_type,
                            'is_classification': self.is_classification,
                            'detection_method': 'simple_mapping'
                        }
                        
                        self.discovered_rules = simple_mapping_rules
                        self._display_optimal_results(simple_mapping_rules)
                        return simple_mapping_rules
            
            # 8. ç‰¹å¾ç»„åˆç­–ç•¥ (å¦‚æœç®€å•æ˜ å°„ä¸è¶³å¤Ÿå¥½ï¼Œåˆ™è¿›è¡Œå¤æ‚æœç´¢)
            if manual_split_features is not None and manual_poly_features is not None:
                # ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç‰¹å¾ç»„åˆ
                combinations_to_try = [(manual_split_features, manual_poly_features)]
                print(f"ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç‰¹å¾ç»„åˆ")
            elif self.enable_exhaustive_search:
                # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾ç»„åˆ
                combinations_to_try = self._generate_feature_combinations(numeric_features, categorical_features, target_col, effective_poly_candidates)
                print(f"ç”Ÿæˆ {len(combinations_to_try)} ä¸ªç‰¹å¾ç»„åˆè¿›è¡Œç©·ä¸¾æœç´¢")
            else:
                # ä½¿ç”¨å¯å‘å¼æ–¹æ³•ç”Ÿæˆå°‘é‡é«˜è´¨é‡ç»„åˆ
                all_combinations = self._generate_feature_combinations(numeric_features, categorical_features, target_col, effective_poly_candidates)
                combinations_to_try = self._select_promising_combinations(all_combinations, all_split_candidates, effective_poly_candidates)
                print(f"ä½¿ç”¨å¯å‘å¼æ–¹æ³•é€‰æ‹© {len(combinations_to_try)} ä¸ªç‰¹å¾ç»„åˆ")
            
            # 9. è¯„ä¼°æ‰€æœ‰ç»„åˆ
            print("\nğŸ” å¼€å§‹è¯„ä¼°ç‰¹å¾ç»„åˆ...")
            best_score = -1
            best_rules = []
            progress_interval = max(1, len(combinations_to_try) // 10)
            
            for i, (split_features, poly_features) in enumerate(combinations_to_try):
                # åªåœ¨å…³é”®è¿›åº¦ç‚¹æ˜¾ç¤ºä¿¡æ¯
                if i % progress_interval == 0 or i == len(combinations_to_try) - 1:
                    progress = (i + 1) / len(combinations_to_try) * 100
                    print(f"   è¿›åº¦: {progress:.1f}% ({i+1}/{len(combinations_to_try)}) - å½“å‰æœ€ä½³åˆ†æ•°: {best_score:.3f}")
                
                score, rules = self._evaluate_combination(encoded_data, split_features, poly_features, target_col, encoded_target, target_info)
                
                if score > best_score:
                    previous_score = best_score
                    best_score = score
                    best_rules = rules
                    self.best_configuration = {
                        'split_features': split_features,
                        'poly_features': poly_features,
                        'score': score,
                        'num_rules': len(rules),
                        'target_type': self.target_type,
                        'is_classification': self.is_classification
                    }
                    
                    # åªåœ¨æ‰¾åˆ°æ˜æ˜¾æ›´å¥½çš„ç»„åˆæ—¶æ‰è¾“å‡º
                    if previous_score <= 0 or score > previous_score * 1.05:  # æå‡è¶…è¿‡5%æ‰æŠ¥å‘Š
                        print(f"   âœ¨ å‘ç°æ›´ä¼˜ç»„åˆ! åˆ†æ®µç‰¹å¾: {split_features}")
                        print(f"      å¤šé¡¹å¼ç‰¹å¾: {poly_features}")
                        print(f"      è¯„åˆ†æå‡: {score:.3f} (ä¹‹å‰: {previous_score:.3f})")
            
            # 10. è¾“å‡ºç»“æœ
            elapsed_time = time.time() - start_time
            print(f"\nâœ… æœç´¢å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            if best_rules:
                print(f"\nğŸ† æœ€ä¼˜ç‰¹å¾é…ç½®:")
                print(f"   ğŸ¯ é—®é¢˜ç±»å‹: {'åˆ†ç±»é—®é¢˜' if self.is_classification else 'å›å½’é—®é¢˜'}")
                print(f"   ğŸ”§ åˆ†æ®µç‰¹å¾: {self.best_configuration['split_features']}")
                print(f"   ğŸ“Š {'æ˜ å°„ç‰¹å¾' if self.is_classification else 'å¤šé¡¹å¼ç‰¹å¾'}: {self.best_configuration['poly_features']}")
                print(f"   ğŸ“ˆ ç»¼åˆè¯„åˆ†: {self.best_configuration['score']:.3f}")
                print(f"   ğŸ“‹ å‘ç°è§„åˆ™æ•°: {self.best_configuration['num_rules']}")
                
                self._display_optimal_results(best_rules)
            else:
                print("âŒ æœªå‘ç°æœ‰æ•ˆçš„æ¡ä»¶è§„åˆ™")
                print("ğŸ’¡ å»ºè®®:")
                print("   â€¢ å°è¯•å‡å° --min-samples å‚æ•°")
                print("   â€¢ å°è¯•å¢å¤§ --max-depth å‚æ•°")
                if not self.is_classification:
                    print("   â€¢ æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ•°å€¼ç‰¹å¾")
            
            return best_rules
            
        except FileNotFoundError:
            print(f"é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° {csv_file_path}")
            return []
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _display_optimal_results(self, rules):
        """æ˜¾ç¤ºæœ€ä¼˜ç»“æœ"""
        if not rules:
            print("\næœªå‘ç°ä»»ä½•æœ‰æ•ˆè§„åˆ™")
            return
            
        print(f"\n{'='*50} æœ€ä¼˜è§„åˆ™è¯¦æƒ… {'='*50}")
        
        # ğŸ†• åŒºåˆ†åˆ†ç±»è§„åˆ™å’Œå›å½’è§„åˆ™
        classification_rules = [r for r in rules if r.get('rule_type') == 'classification']
        regression_rules = [r for r in rules if r.get('rule_type') == 'regression']
        
        # æŒ‰è¯„åˆ†æ’åºï¼Œå»é‡
        unique_rules = []
        seen_rules = set()
        
        for rule in rules:
            rule_key = (rule['condition'], rule['rule'])
            if rule_key not in seen_rules:
                seen_rules.add(rule_key)
                unique_rules.append(rule)
        
        # ğŸ†• æ ¹æ®è§„åˆ™ç±»å‹é€‰æ‹©åˆé€‚çš„æ’åºæ–¹å¼
        if self.is_classification:
            # åˆ†ç±»è§„åˆ™ï¼šæŒ‰å‡†ç¡®ç‡æ’åº
            sorted_rules = sorted(unique_rules, key=lambda x: x['score'], reverse=True)
            score_name = "å‡†ç¡®ç‡"
        else:
            # å›å½’è§„åˆ™ï¼šæŒ‰RÂ²æ’åº
            sorted_rules = sorted(unique_rules, key=lambda x: x.get('cv_r2_score', x['score']), reverse=True)
            score_name = "RÂ²"
        
        # æ˜¾ç¤ºè¯¦ç»†è§„åˆ™ä¿¡æ¯
        for i, rule in enumerate(sorted_rules, 1):
            rule_type_icon = "ğŸ¯" if rule.get('rule_type') == 'classification' else "ğŸ“ˆ"
            print(f"\n{rule_type_icon} è§„åˆ™ {i} ({'åˆ†ç±»è§„åˆ™' if rule.get('rule_type') == 'classification' else 'å›å½’è§„åˆ™'}):")
            print(f"  æ¡ä»¶: {rule['condition']}")
            print(f"  è§„åˆ™: {rule['rule']}")
            
            if rule.get('rule_type') == 'classification':
                print(f"  å‡†ç¡®ç‡: {rule['score']:.3f}")
                if 'target_value' in rule:
                    print(f"  ç›®æ ‡å€¼: {rule['target_value']}")
            else:
                score_key = 'cv_r2_score' if 'cv_r2_score' in rule else 'score'
                print(f"  {score_name}: {rule[score_key]:.3f}")
                
            print(f"  æ ·æœ¬æ•°: {rule['sample_count']}")
            if i < len(sorted_rules):  # ä¸æ˜¯æœ€åä¸€ä¸ªè§„åˆ™
                print("  " + "-" * 60)
        
        # ç”Ÿæˆæ¸…æ™°çš„è¡¨æ ¼æ€»ç»“
        print(f"\n{'='*50} æœ€ä¼˜è§„åˆ™æ±‡æ€»è¡¨ {'='*50}")
        
        # åŠ¨æ€è°ƒæ•´åˆ—å®½
        max_condition_len = min(50, max(len(rule['condition']) for rule in sorted_rules) + 2)
        max_rule_len = min(40, max(len(rule['rule']) for rule in sorted_rules) + 2)
        
        # æ‰“å°è¡¨å¤´
        score_header = "å‡†ç¡®ç‡" if self.is_classification else "RÂ²"
        header = f"| {'æ’å':^4} | {'æ¡ä»¶':^{max_condition_len}} | {'è§„åˆ™':^{max_rule_len}} | {score_header:^6} | {'æ ·æœ¬æ•°':^6} | {'ç±»å‹':^4} |"
        separator = "|" + "-" * 6 + "|" + "-" * (max_condition_len + 2) + "|" + "-" * (max_rule_len + 2) + "|" + "-" * 8 + "|" + "-" * 8 + "|" + "-" * 6 + "|"
        
        print(separator)
        print(header)
        print(separator)
        
        # æ‰“å°è§„åˆ™è¡Œ
        for i, rule in enumerate(sorted_rules, 1):
            condition = rule['condition']
            if len(condition) > max_condition_len:
                condition = condition[:max_condition_len-3] + "..."
                
            rule_str = rule['rule']
            if len(rule_str) > max_rule_len:
                rule_str = rule_str[:max_rule_len-3] + "..."
            
            # ğŸ†• æ ¹æ®è§„åˆ™ç±»å‹æ˜¾ç¤ºä¸åŒçš„åˆ†æ•°
            if rule.get('rule_type') == 'classification':
                score_val = rule['score']
                rule_type_short = "åˆ†ç±»"
            else:
                score_val = rule.get('cv_r2_score', rule['score'])
                rule_type_short = "å›å½’"
            
            row = f"| {i:^4} | {condition:<{max_condition_len}} | {rule_str:<{max_rule_len}} | {score_val:^6.3f} | {rule['sample_count']:^6} | {rule_type_short:^4} |"
            print(row)
        
        print(separator)
        
        # ğŸ†• åˆ†ç±»å‹ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š è§„åˆ™ç»Ÿè®¡:")
        print(f"   â€¢ æ€»è§„åˆ™æ•°: {len(sorted_rules)}")
        
        if classification_rules:
            avg_accuracy = np.mean([r['score'] for r in classification_rules])
            print(f"   â€¢ åˆ†ç±»è§„åˆ™æ•°: {len(classification_rules)}")
            print(f"   â€¢ å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.3f}")
            
            # åˆ†ç±»è§„åˆ™è´¨é‡åˆ†çº§
            excellent_class_rules = [r for r in classification_rules if r['score'] >= 0.95]
            good_class_rules = [r for r in classification_rules if 0.8 <= r['score'] < 0.95]
            fair_class_rules = [r for r in classification_rules if r['score'] < 0.8]
            
            print(f"   â€¢ ä¼˜ç§€åˆ†ç±»è§„åˆ™(å‡†ç¡®ç‡â‰¥95%): {len(excellent_class_rules)}æ¡")
            print(f"   â€¢ è‰¯å¥½åˆ†ç±»è§„åˆ™(80%â‰¤å‡†ç¡®ç‡<95%): {len(good_class_rules)}æ¡")
            print(f"   â€¢ ä¸€èˆ¬åˆ†ç±»è§„åˆ™(å‡†ç¡®ç‡<80%): {len(fair_class_rules)}æ¡")
        
        if regression_rules:
            avg_r2 = np.mean([r.get('cv_r2_score', r['score']) for r in regression_rules])
            print(f"   â€¢ å›å½’è§„åˆ™æ•°: {len(regression_rules)}")
            print(f"   â€¢ å¹³å‡RÂ²åˆ†æ•°: {avg_r2:.3f}")
            
            # å›å½’è§„åˆ™è´¨é‡åˆ†çº§
            excellent_reg_rules = [r for r in regression_rules if r.get('cv_r2_score', r['score']) >= 0.9]
            good_reg_rules = [r for r in regression_rules if 0.7 <= r.get('cv_r2_score', r['score']) < 0.9]
            fair_reg_rules = [r for r in regression_rules if r.get('cv_r2_score', r['score']) < 0.7]
            
            print(f"   â€¢ ä¼˜ç§€å›å½’è§„åˆ™(RÂ²â‰¥0.9): {len(excellent_reg_rules)}æ¡")
            print(f"   â€¢ è‰¯å¥½å›å½’è§„åˆ™(0.7â‰¤RÂ²<0.9): {len(good_reg_rules)}æ¡")
            print(f"   â€¢ ä¸€èˆ¬å›å½’è§„åˆ™(RÂ²<0.7): {len(fair_reg_rules)}æ¡")
        
        print(f"   â€¢ è¦†ç›–æ ·æœ¬æ€»æ•°: {sum(r['sample_count'] for r in sorted_rules)}")
        
        # ğŸ”— æ™ºèƒ½åˆå¹¶ç›¸åŒè§„åˆ™
        merged_rules = self._merge_similar_rules(sorted_rules)
        
        if len(merged_rules) < len(sorted_rules):
            # æ˜¾ç¤ºåˆå¹¶åçš„ç»“æœ
            print(f"\n{'='*50} æ™ºèƒ½åˆå¹¶åè§„åˆ™ {'='*50}")
            
            # é‡æ–°è®¡ç®—åˆå¹¶åè§„åˆ™çš„è¡¨æ ¼å®½åº¦
            merged_max_condition_len = min(60, max(len(rule['condition']) for rule in merged_rules) + 2)
            merged_max_rule_len = min(40, max(len(rule['rule']) for rule in merged_rules) + 2)
            
            # åˆå¹¶åè¡¨æ ¼
            merged_header = f"| {'æ’å':^4} | {'æ¡ä»¶':^{merged_max_condition_len}} | {'è§„åˆ™':^{merged_max_rule_len}} | {score_header:^6} | {'æ ·æœ¬æ•°':^6} | {'ç±»å‹':^4} | {'åˆå¹¶æ•°':^6} |"
            merged_separator = "|" + "-" * 6 + "|" + "-" * (merged_max_condition_len + 2) + "|" + "-" * (merged_max_rule_len + 2) + "|" + "-" * 8 + "|" + "-" * 8 + "|" + "-" * 6 + "|" + "-" * 8 + "|"
            
            print(merged_separator)
            print(merged_header)
            print(merged_separator)
            
            # æŒ‰è¯„åˆ†é‡æ–°æ’åºåˆå¹¶åçš„è§„åˆ™
            merged_sorted = sorted(merged_rules, key=lambda x: x['score'], reverse=True)
            
            for i, rule in enumerate(merged_sorted, 1):
                condition = rule['condition']
                if len(condition) > merged_max_condition_len:
                    condition = condition[:merged_max_condition_len-3] + "..."
                    
                rule_str = rule['rule']
                if len(rule_str) > merged_max_rule_len:
                    rule_str = rule_str[:merged_max_rule_len-3] + "..."
                
                rule_type_short = "åˆ†ç±»" if rule.get('rule_type') == 'classification' else "å›å½’"
                merge_info = f"{rule.get('merged_from', 1)}" if 'merged_from' in rule else "1"
                
                row = f"| {i:^4} | {condition:<{merged_max_condition_len}} | {rule_str:<{merged_max_rule_len}} | {rule['score']:^6.3f} | {rule['sample_count']:^6} | {rule_type_short:^4} | {merge_info:^6} |"
                print(row)
            
            print(merged_separator)
            
            # åˆå¹¶åç»Ÿè®¡
            print(f"\nğŸ“Š åˆå¹¶åè§„åˆ™ç»Ÿè®¡:")
            print(f"   â€¢ åˆå¹¶åè§„åˆ™æ•°: {len(merged_rules)} (å‡å°‘äº† {len(sorted_rules) - len(merged_rules)} æ¡)")
            print(f"   â€¢ å¹³å‡è¯„åˆ†: {np.mean([r['score'] for r in merged_rules]):.3f}")
            print(f"   â€¢ è¦†ç›–æ ·æœ¬æ€»æ•°: {sum(r['sample_count'] for r in merged_rules)}")
            
            # çªå‡ºæ˜¾ç¤ºç®€åŒ–æ•ˆæœ
            simplification_rate = (len(sorted_rules) - len(merged_rules)) / len(sorted_rules) * 100
            print(f"   ğŸ¯ è§„åˆ™ç®€åŒ–ç‡: {simplification_rate:.1f}%")
            
            # ğŸ’¾ ä¿å­˜å‘ç°çš„è§„åˆ™ä¾›é¢„æµ‹å™¨ä½¿ç”¨
            self.discovered_rules = merged_rules
            print(f"\nâœ… è§„åˆ™å·²ä¿å­˜åˆ°å‘ç°å™¨ï¼Œå¯ç”¨äºåç»­é¢„æµ‹")
            
        else:
            print(f"\nğŸ’¡ æ‰€æœ‰è§„åˆ™éƒ½æ˜¯å”¯ä¸€çš„ï¼Œæ— éœ€åˆå¹¶")
            # ğŸ’¾ ä¿å­˜å‘ç°çš„è§„åˆ™ä¾›é¢„æµ‹å™¨ä½¿ç”¨
            self.discovered_rules = sorted_rules
            print(f"\nâœ… è§„åˆ™å·²ä¿å­˜åˆ°å‘ç°å™¨ï¼Œå¯ç”¨äºåç»­é¢„æµ‹")
            
        return merged_rules if len(merged_rules) < len(sorted_rules) else sorted_rules
    
    def _merge_similar_rules(self, rules):
        """
        åˆå¹¶ç›¸åŒè§„åˆ™çš„æ¡ä»¶
        
        Args:
            rules: è§„åˆ™åˆ—è¡¨
            
        Returns:
            merged_rules: åˆå¹¶åçš„è§„åˆ™åˆ—è¡¨
        """
        if not rules:
            return []
            
        print(f"\nğŸ”— å¼€å§‹æ™ºèƒ½åˆå¹¶ç›¸åŒè§„åˆ™...")
        
        # æŒ‰è§„åˆ™å†…å®¹åˆ†ç»„
        rule_groups = {}
        for rule in rules:
            rule_formula = rule['rule']
            if rule_formula not in rule_groups:
                rule_groups[rule_formula] = []
            rule_groups[rule_formula].append(rule)
        
        merged_rules = []
        merge_count = 0
        
        for rule_formula, group_rules in rule_groups.items():
            if len(group_rules) == 1:
                # åªæœ‰ä¸€ä¸ªè§„åˆ™ï¼Œç›´æ¥ä¿ç•™
                merged_rules.append(group_rules[0])
            else:
                # ğŸ”§ ä¿®å¤ï¼šä¸å†è‡ªåŠ¨åˆå¹¶ï¼Œè€Œæ˜¯æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆå¹¶
                print(f"   æ£€æŸ¥è§„åˆ™: {rule_formula}")
                print(f"   å‘ç° {len(group_rules)} ä¸ªç›¸åŒå…¬å¼çš„æ¡ä»¶")
                
                # æ£€æŸ¥æ¡ä»¶æ˜¯å¦çœŸçš„å¯ä»¥åˆå¹¶ï¼ˆä¸äº§ç”ŸçŸ›ç›¾ï¼‰
                can_merge = self._can_merge_conditions_safely(group_rules)
                
                if can_merge:
                    merged_rule = self._merge_conditions(group_rules, rule_formula)
                    merged_rules.append(merged_rule)
                    merge_count += len(group_rules) - 1
                    print(f"   âœ… å®‰å…¨åˆå¹¶å®Œæˆï¼Œæ¡ä»¶: {merged_rule['condition']}")
                else:
                    # ä¸èƒ½å®‰å…¨åˆå¹¶ï¼Œä¿ç•™æ‰€æœ‰ç‹¬ç«‹è§„åˆ™
                    merged_rules.extend(group_rules)
                    print(f"   âš ï¸ æ¡ä»¶å­˜åœ¨å†²çªï¼Œä¿ç•™ {len(group_rules)} ä¸ªç‹¬ç«‹è§„åˆ™")
        
        print(f"ğŸ¯ åˆå¹¶ç»Ÿè®¡: åŸæœ‰ {len(rules)} æ¡è§„åˆ™ï¼Œå¤„ç†å {len(merged_rules)} æ¡è§„åˆ™ï¼Œå®é™…åˆå¹¶äº† {merge_count} æ¡")
        return merged_rules
    
    def _can_merge_conditions_safely(self, group_rules):
        """
        æ£€æŸ¥ä¸€ç»„è§„åˆ™çš„æ¡ä»¶æ˜¯å¦å¯ä»¥å®‰å…¨åˆå¹¶ï¼ˆä¸äº§ç”Ÿé€»è¾‘çŸ›ç›¾ï¼‰
        
        Args:
            group_rules: ç›¸åŒè§„åˆ™å…¬å¼çš„è§„åˆ™åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦å¯ä»¥å®‰å…¨åˆå¹¶
        """
        if len(group_rules) <= 1:
            return True
        
        # è§£ææ‰€æœ‰æ¡ä»¶
        all_conditions = []
        for rule in group_rules:
            conditions = self._parse_condition(rule['condition'])
            all_conditions.append(conditions)
        
        # æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„æ¡ä»¶æ˜¯å¦å¯ä»¥å…¼å®¹
        all_features = set()
        for conditions in all_conditions:
            all_features.update(conditions.keys())
        
        for feature in all_features:
            feature_ranges = []
            
            # æ”¶é›†è¯¥ç‰¹å¾çš„æ‰€æœ‰çº¦æŸ
            for conditions in all_conditions:
                if feature in conditions:
                    cond = conditions[feature]
                    if cond['type'] == 'numeric':
                        lower = cond.get('lower')
                        upper = cond.get('upper')
                        
                        # æ„å»ºèŒƒå›´
                        if lower is not None and upper is not None:
                            if lower >= upper:  # çŸ›ç›¾èŒƒå›´
                                return False
                            feature_ranges.append((lower, upper))
                        elif lower is not None:
                            feature_ranges.append((lower, float('inf')))
                        elif upper is not None:
                            feature_ranges.append((float('-inf'), upper))
            
            # æ£€æŸ¥èŒƒå›´æ˜¯å¦æœ‰åˆç†çš„äº¤é›†
            if len(feature_ranges) > 1:
                # è®¡ç®—æ‰€æœ‰èŒƒå›´çš„äº¤é›†
                intersection = feature_ranges[0]
                for rng in feature_ranges[1:]:
                    # è®¡ç®—äº¤é›†
                    new_lower = max(intersection[0], rng[0])
                    new_upper = min(intersection[1], rng[1])
                    
                    if new_lower >= new_upper:  # æ²¡æœ‰æœ‰æ•ˆäº¤é›†
                        return False
                    
                    intersection = (new_lower, new_upper)
        
        return True
    
    def _merge_conditions(self, group_rules, rule_formula):
        """
        åˆå¹¶ä¸€ç»„ç›¸åŒè§„åˆ™çš„æ¡ä»¶
        
        Args:
            group_rules: ç›¸åŒè§„åˆ™çš„åˆ—è¡¨
            rule_formula: è§„åˆ™å…¬å¼
            
        Returns:
            merged_rule: åˆå¹¶åçš„è§„åˆ™
        """
        # è§£ææ‰€æœ‰æ¡ä»¶
        all_conditions = []
        total_samples = 0
        total_r2_weighted = 0
        
        for rule in group_rules:
            conditions = self._parse_condition(rule['condition'])
            all_conditions.append(conditions)
            total_samples += rule['sample_count']
            total_r2_weighted += rule['cv_r2_score'] * rule['sample_count']
        
        # è®¡ç®—åŠ æƒå¹³å‡RÂ²
        avg_r2 = total_r2_weighted / total_samples if total_samples > 0 else 0
        
        # åˆå¹¶æ¡ä»¶
        merged_conditions = self._merge_condition_logic(all_conditions)
        merged_condition_str = self._format_merged_conditions(merged_conditions)
        
        return {
            'condition': merged_condition_str,
            'rule': rule_formula,
            'cv_r2_score': avg_r2,
            'sample_count': total_samples,
            'merged_from': len(group_rules)
        }
    
    def _parse_condition(self, condition_str):
        """
        è§£ææ¡ä»¶å­—ç¬¦ä¸²ä¸ºç»“æ„åŒ–æ ¼å¼
        
        Args:
            condition_str: æ¡ä»¶å­—ç¬¦ä¸²ï¼Œå¦‚ "x <= 39.50 ä¸” y âˆˆ {y1}" æˆ– "29.50 < x <= 39.50"
            
        Returns:
            conditions: è§£æåçš„æ¡ä»¶å­—å…¸
        """
        conditions = {}
        
        # æŒ‰ "ä¸”" åˆ†å‰²æ¡ä»¶
        parts = condition_str.split(' ä¸” ')
        
        for part in parts:
            part = part.strip()
            
            if ' âˆˆ ' in part:
                # åˆ†ç±»æ¡ä»¶ï¼šy âˆˆ {y1, y2}
                feature, values_str = part.split(' âˆˆ ')
                feature = feature.strip()
                # æå–é›†åˆä¸­çš„å€¼
                values_str = values_str.strip().replace('{', '').replace('}', '')
                values = [v.strip() for v in values_str.split(',')]
                
                if feature not in conditions:
                    conditions[feature] = {'type': 'categorical', 'values': set()}
                conditions[feature]['values'].update(values)
                
            elif '<' in part and '<=' in part:
                # èŒƒå›´æ¡ä»¶ï¼š29.50 < x <= 39.50
                import re
                match = re.match(r'(\d+\.?\d*)\s*<\s*(\w+)\s*<=\s*(\d+\.?\d*)', part)
                if match:
                    lower_val, feature, upper_val = match.groups()
                    feature = feature.strip()
                    lower = float(lower_val)
                    upper = float(upper_val)
                    
                    if feature not in conditions:
                        conditions[feature] = {'type': 'numeric', 'upper': None, 'lower': None}
                    
                    # è®¾ç½®èŒƒå›´è¾¹ç•Œ
                    if conditions[feature]['lower'] is None or lower > conditions[feature]['lower']:
                        conditions[feature]['lower'] = lower
                    if conditions[feature]['upper'] is None or upper < conditions[feature]['upper']:
                        conditions[feature]['upper'] = upper
                        
            elif '<=' in part and '<' not in part:
                # å•è¾¹æ¡ä»¶ï¼šx <= 39.50
                feature, value_str = part.split('<=')
                feature = feature.strip()
                value = float(value_str.strip())
                
                if feature not in conditions:
                    conditions[feature] = {'type': 'numeric', 'upper': None, 'lower': None}
                
                if conditions[feature]['upper'] is None or value < conditions[feature]['upper']:
                    conditions[feature]['upper'] = value
                    
            elif '>' in part and not ('<' in part and '<=' in part):
                # å•è¾¹æ¡ä»¶ï¼šx > 39.50
                feature, value_str = part.split('>')
                feature = feature.strip()
                value = float(value_str.strip())
                
                if feature not in conditions:
                    conditions[feature] = {'type': 'numeric', 'upper': None, 'lower': None}
                
                if conditions[feature]['lower'] is None or value > conditions[feature]['lower']:
                    conditions[feature]['lower'] = value
        
        return conditions
    
    def _merge_condition_logic(self, all_conditions):
        """
        åˆå¹¶å¤šä¸ªæ¡ä»¶çš„é€»è¾‘
        
        Args:
            all_conditions: å¤šä¸ªæ¡ä»¶çš„åˆ—è¡¨
            
        Returns:
            merged_conditions: åˆå¹¶åçš„æ¡ä»¶
        """
        merged = {}
        
        # æ”¶é›†æ‰€æœ‰ç‰¹å¾
        all_features = set()
        for conditions in all_conditions:
            all_features.update(conditions.keys())
        
        for feature in all_features:
            feature_conditions = []
            
            # æ”¶é›†è¯¥ç‰¹å¾çš„æ‰€æœ‰æ¡ä»¶
            for conditions in all_conditions:
                if feature in conditions:
                    feature_conditions.append(conditions[feature])
            
            if not feature_conditions:
                continue
                
            first_condition = feature_conditions[0]
            
            if first_condition['type'] == 'categorical':
                # åˆ†ç±»ç‰¹å¾ï¼šå–å¹¶é›†
                all_values = set()
                for cond in feature_conditions:
                    all_values.update(cond['values'])
                
                merged[feature] = {
                    'type': 'categorical',
                    'values': all_values
                }
                
            elif first_condition['type'] == 'numeric':
                # æ•°å€¼ç‰¹å¾ï¼šè®¡ç®—çœŸæ­£çš„å¹¶é›†èŒƒå›´
                ranges = []
                
                # æ”¶é›†æ‰€æœ‰çš„æ•°å€¼èŒƒå›´
                for cond in feature_conditions:
                    upper = cond.get('upper')
                    lower = cond.get('lower')
                    
                    # æ„å»ºèŒƒå›´å…ƒç»„ (lower_bound, upper_bound, inclusive_lower, inclusive_upper)
                    if upper is not None and lower is not None:
                        # å½¢å¦‚ lower < x <= upper
                        ranges.append((lower, upper, False, True))
                    elif upper is not None:
                        # å½¢å¦‚ x <= upperï¼Œç›¸å½“äº (-âˆ, upper]
                        ranges.append((float('-inf'), upper, True, True))
                    elif lower is not None:
                        # å½¢å¦‚ x > lowerï¼Œç›¸å½“äº (lower, +âˆ)
                        ranges.append((lower, float('inf'), False, True))
                
                # åˆå¹¶é‡å çš„èŒƒå›´
                if ranges:
                    merged_range = self._merge_numeric_ranges(ranges)
                    
                    # å°†åˆå¹¶åçš„èŒƒå›´è½¬æ¢å›æ¡ä»¶æ ¼å¼
                    if len(merged_range) == 1:
                        lower, upper, incl_lower, incl_upper = merged_range[0]
                        
                        final_condition = {'type': 'numeric', 'upper': None, 'lower': None}
                        
                        if lower != float('-inf'):
                            final_condition['lower'] = lower
                        if upper != float('inf'):
                            final_condition['upper'] = upper
                            
                        merged[feature] = final_condition
                    else:
                        # å¤šä¸ªä¸è¿ç»­çš„èŒƒå›´ï¼Œæš‚æ—¶å–ç¬¬ä¸€ä¸ªèŒƒå›´ï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
                        lower, upper, incl_lower, incl_upper = merged_range[0]
                        final_condition = {'type': 'numeric', 'upper': None, 'lower': None}
                        
                        if lower != float('-inf'):
                            final_condition['lower'] = lower
                        if upper != float('inf'):
                            final_condition['upper'] = upper
                            
                        merged[feature] = final_condition
        
        return merged
    
    def _merge_numeric_ranges(self, ranges):
        """
        åˆå¹¶æ•°å€¼èŒƒå›´
        
        Args:
            ranges: èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (lower, upper, incl_lower, incl_upper)
            
        Returns:
            merged_ranges: åˆå¹¶åçš„èŒƒå›´åˆ—è¡¨
        """
        if not ranges:
            return []
        
        # æ’åºèŒƒå›´
        sorted_ranges = sorted(ranges, key=lambda x: (x[0], x[1]))
        
        merged = [sorted_ranges[0]]
        
        for current in sorted_ranges[1:]:
            last = merged[-1]
            
            # æ£€æŸ¥æ˜¯å¦é‡å æˆ–ç›¸é‚»
            if current[0] <= last[1]:  # æœ‰é‡å 
                # åˆå¹¶èŒƒå›´
                new_lower = min(last[0], current[0])
                new_upper = max(last[1], current[1])
                merged[-1] = (new_lower, new_upper, True, True)
            else:
                # æ²¡æœ‰é‡å ï¼Œæ·»åŠ æ–°èŒƒå›´
                merged.append(current)
        
        return merged
    
    def _format_merged_conditions(self, merged_conditions):
        """
        æ ¼å¼åŒ–åˆå¹¶åçš„æ¡ä»¶
        
        Args:
            merged_conditions: åˆå¹¶åçš„æ¡ä»¶å­—å…¸
            
        Returns:
            condition_str: æ ¼å¼åŒ–çš„æ¡ä»¶å­—ç¬¦ä¸²
        """
        condition_parts = []
        
        for feature, condition in merged_conditions.items():
            if condition['type'] == 'categorical':
                values_str = ', '.join(sorted(condition['values']))
                condition_parts.append(f"{feature} âˆˆ {{{values_str}}}")
                
            elif condition['type'] == 'numeric':
                if condition['lower'] is not None and condition['upper'] is not None:
                    # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥çŸ›ç›¾æ¡ä»¶å¹¶æä¾›åˆç†å¤„ç†
                    if condition['lower'] >= condition['upper']:
                        # çŸ›ç›¾æ¡ä»¶ï¼šè®°å½•è­¦å‘Šä½†ä¸è·³è¿‡ï¼Œè€Œæ˜¯é€‰æ‹©æ›´åˆç†çš„æ¡ä»¶
                        print(f"   âš ï¸ æ£€æµ‹åˆ°çŸ›ç›¾æ¡ä»¶: {feature} > {condition['lower']:.2f} ä¸” {feature} <= {condition['upper']:.2f}")
                        
                        # é€‰æ‹©ä¸­é—´å€¼ä½œä¸ºå•ç‚¹æ¡ä»¶ï¼Œæˆ–è€…é€‰æ‹©æ›´åˆç†çš„è¾¹ç•Œ
                        if abs(condition['lower'] - condition['upper']) < 0.01:
                            # å¦‚æœå·®å€¼å¾ˆå°ï¼Œå¯èƒ½æ˜¯æµ®ç‚¹ç²¾åº¦é—®é¢˜ï¼Œä½¿ç”¨çº¦ç­‰äºæ¡ä»¶
                            mid_val = (condition['lower'] + condition['upper']) / 2
                            condition_parts.append(f"{feature} â‰ˆ {mid_val:.2f}")
                        else:
                            # å·®å€¼è¾ƒå¤§ï¼Œè¿™ç¡®å®æ˜¯ä¸ªçŸ›ç›¾ï¼Œå¯èƒ½éœ€è¦æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹è§„åˆ™
                            # æš‚æ—¶è·³è¿‡è¿™ä¸ªç‰¹å¾çš„çº¦æŸï¼Œä½†è®°å½•é”™è¯¯
                            print(f"   âŒ ç‰¹å¾ {feature} çš„æ¡ä»¶å­˜åœ¨ä¸¥é‡çŸ›ç›¾ï¼Œè·³è¿‡æ­¤çº¦æŸ")
                            continue
                    else:
                        condition_parts.append(f"{condition['lower']:.2f} < {feature} <= {condition['upper']:.2f}")
                elif condition['upper'] is not None:
                    condition_parts.append(f"{feature} <= {condition['upper']:.2f}")
                elif condition['lower'] is not None:
                    condition_parts.append(f"{feature} > {condition['lower']:.2f}")
                else:
                    # æ—¢æ²¡æœ‰ä¸Šç•Œä¹Ÿæ²¡æœ‰ä¸‹ç•Œï¼Œè¿™ä¸ªæ¡ä»¶æ— æ•ˆ
                    print(f"   âš ï¸ ç‰¹å¾ {feature} æ²¡æœ‰æœ‰æ•ˆçš„æ•°å€¼çº¦æŸ")
        
        result = ' ä¸” '.join(condition_parts)
        
        # ğŸ”§ æ–°å¢ï¼šéªŒè¯ç»“æœæ¡ä»¶çš„åˆç†æ€§
        if not result or len(condition_parts) == 0:
            print(f"   âŒ è­¦å‘Šï¼šç”Ÿæˆçš„æ¡ä»¶ä¸ºç©ºæˆ–æ— æ•ˆ")
            
        return result

def discover_optimal_conditional_rules(csv_file_path, target_col=None,
                                      manual_split_features=None, manual_poly_features=None,
                                      max_depth=3, min_samples_leaf=50, 
                                      enable_exhaustive_search=True, max_combinations=100):
    """
    ä¼˜åŒ–ç‰ˆæ¡ä»¶å¤šé¡¹å¼è§„åˆ™å‘ç°å‡½æ•°
    
    Args:
        csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        target_col: ç›®æ ‡åˆ—åï¼Œé»˜è®¤ä½¿ç”¨æœ€åä¸€åˆ—
        manual_split_features: æ‰‹åŠ¨æŒ‡å®šåˆ†æ®µç‰¹å¾
        manual_poly_features: æ‰‹åŠ¨æŒ‡å®šå¤šé¡¹å¼ç‰¹å¾
        max_depth: å†³ç­–æ ‘æœ€å¤§æ·±åº¦
        min_samples_leaf: å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
        enable_exhaustive_search: æ˜¯å¦å¯ç”¨ç©·ä¸¾æœç´¢
        max_combinations: æœ€å¤§å°è¯•çš„ç»„åˆæ•°
        
    Returns:
        æœ€ä¼˜è§„åˆ™åˆ—è¡¨
    """
    discoverer = OptimalConditionalRuleDiscoverer(
        max_depth=max_depth,
        min_samples_leaf=min_samples_leaf,
        enable_exhaustive_search=enable_exhaustive_search,
        max_combinations=max_combinations
    )
    
    return discoverer.discover_optimal_rules(
        csv_file_path=csv_file_path,
        target_col=target_col,
        manual_split_features=manual_split_features,
        manual_poly_features=manual_poly_features
    )

if __name__ == "__main__":
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆæ¡ä»¶å¤šé¡¹å¼è§„åˆ™å‘ç°å·¥å…·')
    parser.add_argument('csv_file', type=str, help='è¦åˆ†æçš„CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target-col', type=str, help='ç›®æ ‡åˆ—çš„åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨æœ€åä¸€åˆ—')
    parser.add_argument('--split-features', type=str, nargs='+', help='æ‰‹åŠ¨æŒ‡å®šåˆ†æ®µç‰¹å¾')
    parser.add_argument('--poly-features', type=str, nargs='+', help='æ‰‹åŠ¨æŒ‡å®šå¤šé¡¹å¼ç‰¹å¾')
    parser.add_argument('--max-depth', type=int, default=3, help='å†³ç­–æ ‘æœ€å¤§æ·±åº¦')
    parser.add_argument('--min-samples', type=int, default=50, help='å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°')
    parser.add_argument('--max-combinations', type=int, default=100, help='æœ€å¤§å°è¯•çš„ç‰¹å¾ç»„åˆæ•°')
    parser.add_argument('--disable-exhaustive', action='store_true', help='ç¦ç”¨ç©·ä¸¾æœç´¢ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    print("ğŸš€ === ä¼˜åŒ–ç‰ˆæ¡ä»¶è§„åˆ™å‘ç°å·¥å…· === ğŸš€")
    print("ä¸»è¦ç‰¹æ€§:")
    print("  âœ“ æ™ºèƒ½ç‰¹å¾ç»„åˆç©·ä¸¾")
    print("  âœ“ äº¤å‰éªŒè¯è¯„ä¼°æœ€ä¼˜ç»„åˆ")
    print("  âœ“ å¯å‘å¼æœç´¢ä¼˜åŒ–")
    print("  âœ“ åŠ¨æ€ç‰¹å¾åˆ†é…")
    print("  âœ“ æ”¯æŒåˆ†ç±»å’Œæ•°å€¼ç‰¹å¾")
    print("-" * 60)
    
    # è¿è¡Œä¼˜åŒ–ç‰ˆè§„åˆ™å‘ç°
    try:
        rules = discover_optimal_conditional_rules(
            csv_file_path=args.csv_file,
            target_col=args.target_col,
            manual_split_features=args.split_features,
            manual_poly_features=args.poly_features,
            max_depth=args.max_depth,
            min_samples_leaf=args.min_samples,
            enable_exhaustive_search=not args.disable_exhaustive,
            max_combinations=args.max_combinations
        )
        
        # æœ€ç»ˆæ€»ç»“
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"   ğŸ“‹ æœ€ç»ˆå‘ç°çš„æœ€ä¼˜è§„åˆ™æ•°é‡: {len(rules)}")
        
        if rules:
            avg_quality = np.mean([r['cv_r2_score'] for r in rules])
            print(f"   ğŸ“ˆ è§„åˆ™å¹³å‡è´¨é‡(RÂ²): {avg_quality:.3f}")
            if avg_quality >= 0.9:
                print("   ğŸŒŸ è§„åˆ™è´¨é‡: ä¼˜ç§€")
            elif avg_quality >= 0.7:
                print("   â­ è§„åˆ™è´¨é‡: è‰¯å¥½") 
            else:
                print("   ğŸ“Š è§„åˆ™è´¨é‡: ä¸€èˆ¬")
        else:
            print("   âš ï¸  å»ºè®®: å°è¯•è°ƒæ•´å‚æ•°æˆ–æ£€æŸ¥æ•°æ®è´¨é‡")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc() 
        
    print("\n" + "="*60)
    print("æ„Ÿè°¢ä½¿ç”¨æ¡ä»¶è§„åˆ™å‘ç°å·¥å…·! ğŸ™") 